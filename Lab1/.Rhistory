install.packages('ElemStatLearn', dep = TRUE)
library(ggplot2) #Import ggplot2 library
library(class)
library(gmodels)
library(ElemStatLearn)
rm(list = ls()) #Clean all the old variables
set.seed(as.numeric(Sys.time())) #Initialization of seed with timestamp to get real random every execution
#LOADING DATASET
irisData = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",header=FALSE,sep = ",") #Loading the dataset in the variable irisData
names(irisData) <- c("SL", "SW" , "PL", "PW", "C" ) #Renaming the colums: SL = sepal length in cm, SW = sepal width in cm, PL = petal length in cm, PW = petal width in cm, C = class
#CHECKING LOADED DATA
irisData
ggplot(irisData, aes(SL, SW, color = C)) + geom_point() + labs(color='Class',x ='Sepal length in cm', y ='Sepal width in cm')
ggplot(irisData, aes(PL, PW, color = C)) + geom_point() + labs(color='Class',x ='Petal length in cm', y ='Petal width in cm')
#KMEAN sepal
set.seed(as.numeric(Sys.time()))
irisClusterSepal <- kmeans(irisData[,1:2], length(unique(irisData[,5])))
irisClusterSepal
table(irisClusterSepal$cluster, irisData$C)
irisClusterSepal$cluster <- as.factor(irisClusterSepal$cluster)
ggplot(irisData, aes(SL, SW, color = irisClusterSepal$cluster,shape=C)) + geom_point() +labs(color='Cluster',shape='Class',x ='Sepal length in cm', y ='Sepal width in cm')
#KMEAN petal
set.seed(as.numeric(Sys.time()))
irisClusterPetal <- kmeans(irisData[,3:4], length(unique(irisData[,5])))
irisClusterPetal
table(irisClusterPetal$cluster, irisData$C)
irisClusterPetal$cluster <- as.factor(irisClusterPetal$cluster)
ggplot(irisData, aes(PL, PW, color = irisClusterPetal$cluster,shape=C)) + geom_point() +labs(color='Cluster',shape='Class',x ='Petal length in cm', y ='Petal width in cm')
#KMEAN petal and sepal
set.seed(as.numeric(Sys.time()))
irisCluster <- kmeans(irisData[,1:4], length(unique(irisData[,5])))
irisCluster
table(irisCluster$cluster, irisData$C)
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(irisData, aes(SL, SW, color = irisCluster$cluster,shape=C)) + geom_point() +labs(color='Cluster', shape='Class',x ='Sepal length in cm', y ='Sepal width in cm')
ggplot(irisData, aes(PL, PW, color = irisCluster$cluster,shape=C)) + geom_point() +labs(color='Cluster', shape='Class',x ='Petal length in cm', y ='Petal width in cm')
# kNN
normalize <- function(x) { return ((x - min(x)) / (max(x) - min(x))) }
iris_normalized <- as.data.frame(lapply(irisData[1:4], normalize))
summary(iris_normalized$sepal_length) # check if normalized
# creating training data from normalized dataframe
iris_train <- rbind(iris_normalized[1:25,], iris_normalized[51:75,],  iris_normalized[101:125,])
iris_train_labels <- c(irisData[1:25,5], irisData[51:75,5],  irisData[101:125,5]) # create a vector of training labels
iris_test <- rbind(iris_normalized[26:50,], iris_normalized[76:100,],  iris_normalized[126:150,])
iris_test_labels <- c(irisData[26:50,5], irisData[76:100,5],  irisData[126:150,5]) # create a vector of test labels
# creating predictions with k=sqrt(total values)
iris_test_pred <- knn(train = iris_train, test = iris_test,cl = iris_train_labels, k=sqrt(nrow(irisData)), prob=TRUE)
CrossTable(x = iris_test_labels, y = iris_test_pred, prop.chisq = FALSE) # print cross table to evaluate predictions
accuracy = function(actual, predicted) { mean(actual == predicted) } # define function to calculate accuracy
accuracy(actual = iris_test_labels, iris_test_pred) # calculate accuracy for the prediction above
# verify k=sqrt(total values) as best value for k
k_max <- nrow(iris_train)/3
results <- vector("list", k_max)
# make knn classifier for each k and write the according accuracy into the results vector
for (k in 1:k_max){
iris_test_pred <- knn(train = iris_train, test = iris_test,cl = iris_train_labels, k=k)
a <- accuracy(actual = iris_test_labels, iris_test_pred)
results[k] <- a
}
# create a dataframe from the vector
df <- data.frame(accuracy=matrix(unlist(results), nrow=k_max, byrow=T))
df$k <- as.numeric(row.names(df)) # write index to dataframe as it represents k in the vector
# plot a line graph of all k values and the corresponding accuracy
ggplot(data=df, aes(x=k, y=accuracy, group=1)) +
geom_line(color = "red")+
geom_point()
prob <- attr(iris_test_pred, "prob")
prob <- ifelse(iris_test_pred=="1", prob, 1-prob)
prob <- attr(iris_test_pred, "prob")
prob <- ifelse(iris_test_pred=="1", prob, 1-prob)
x <- mixture.example$x
g <- mixture.example$y
xnew <- mixture.example$xnew
mod15 <- knn(x, xnew, g, k=15, prob=TRUE)
prob <- attr(mod15, "prob")
prob <- ifelse(mod15=="1", prob, 1-prob)
px1 <- mixture.example$px1
px2 <- mixture.example$px2
prob15 <- matrix(prob, length(px1), length(px2))
par(mar=rep(2,4))
contour(px1, px2, prob15, levels=0.5, labels="", xlab="", ylab="", main=
"15-nearest neighbour", axes=FALSE)
points(x, col=ifelse(g==1, "coral", "cornflowerblue"))
gd <- expand.grid(x=px1, y=px2)
points(gd, pch=".", cex=1.2, col=ifelse(prob15>0.5, "coral", "cornflowerblue"))
dataf <- bind_rows(mutate(test,
prob=prob,
cls="c",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="v",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="s",
prob_cls=ifelse(classif==cls,
1, 0)))
require(dplyr)
install.packages('dplyr', dep = TRUE)
require(dplyr)
dataf <- bind_rows(mutate(test,
prob=prob,
cls="c",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="v",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="s",
prob_cls=ifelse(classif==cls,
1, 0)))
dataf <- bind_rows(mutate(iris_test,
prob=prob,
cls="c",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="v",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="s",
prob_cls=ifelse(classif==cls,
1, 0)))
prob <- attr(iris_test, "prob")
require(dplyr)
dataf <- bind_rows(mutate(iris_test,
prob=prob,
cls="c",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="v",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="s",
prob_cls=ifelse(classif==cls,
1, 0)))
prob <- attr(iris_test_pred, "prob")
require(dplyr)
dataf <- bind_rows(mutate(iris_test_pred,
prob=prob,
cls="c",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="v",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="s",
prob_cls=ifelse(classif==cls,
1, 0)))
prob <- attr(iris_train, "prob")
require(dplyr)
dataf <- bind_rows(mutate(iris_train,
prob=prob,
cls="c",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="v",
prob_cls=ifelse(classif==cls,
1, 0)),
mutate(test,
prob=prob,
cls="s",
prob_cls=ifelse(classif==cls,
1, 0)))
plot.df = data.frame(iris_test, iris_test_pred)
plot.df1 = data.frame(x = plot.df$Sepal.Length,
y = plot.df$Sepal.Width,
predicted = plot.df$predicted)
find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(plot.df1, .variables = "predicted", .fun = find_hull)
install.packages("plyr")
install.packages("plyr")
library(ggplot2) #Import ggplot2 library
library(class)
library(plyr)
plot.df = data.frame(iris_test, iris_test_pred)
plot.df1 = data.frame(x = plot.df$Sepal.Length,
y = plot.df$Sepal.Width,
predicted = plot.df$predicted)
find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(plot.df1, .variables = "predicted", .fun = find_hull)
ggplot(plot.df, aes(Sepal.Length, Sepal.Width, color = predicted, fill = predicted)) +
geom_point(size = 5) +
geom_polygon(data = boundary, aes(x,y), alpha = 0.5)
ggplot(plot.df, aes(SL, SW, color = predicted, fill = predicted)) +
geom_point(size = 5) +
geom_polygon(data = boundary, aes(x,y), alpha = 0.5)
ggplot(plot.df, aes(SL, SW, color = predicted, fill = iris_test_pred)) +
geom_point(size = 5) +
geom_polygon(data = boundary, aes(x,y), alpha = 0.5)
plot.df = data.frame(iris_test, iris_test_pred)
plot.df1 = data.frame(x = plot.df$Sepal.Length,
y = plot.df$Sepal.Width,
predicted = plot.df$predicted)
find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(plot.df1, .variables = "predicted", .fun = find_hull)
ggplot(plot.df, aes(SL, SW, color = predicted, fill = iris_test_pred)) +
geom_point(size = 5) +
geom_polygon(data = boundary, aes(x,y), alpha = 0.5)
plot.df = data.frame(iris_test, iris_test_pred)
plot.df1 = data.frame(x = plot.df$Sepal.Length,
y = plot.df$Sepal.Width,
predicted = plot.df$predicted)
find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(plot.df1, .variables = "predicted", .fun = find_hull)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 5) +
geom_polygon(data = boundary, aes(x,y), alpha = 0.5)
plot.df1 = data.frame(x = plot.df$Sepal.Length,
y = plot.df$Sepal.Width,
predicted = plot.df$predicted)
find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(plot.df1, .variables = "predicted", .fun = find_hull)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 5) +
geom_polygon(data = boundary, aes(x,y), alpha = 0.5)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 5) +
geom_polygon(data = boundary, aes(SL,SW), alpha = 0.5)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 5)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 1)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 1) +
geom_polygon(data = boundary, aes(SL,SW), alpha = 0.5)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 1) +
geom_polygon(data = boundary, aes(x,SW), alpha = 0.5)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 1) +
geom_polygon(data = boundary, aes(x,y), alpha = 0.5)
rm(list = ls()) #Clean all the old variables
set.seed(as.numeric(Sys.time())) #Initialization of seed with timestamp to get real random every execution
#LOADING DATASET
irisData = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",header=FALSE,sep = ",") #Loading the dataset in the variable irisData
names(irisData) <- c("SL", "SW" , "PL", "PW", "C" ) #Renaming the colums: SL = sepal length in cm, SW = sepal width in cm, PL = petal length in cm, PW = petal width in cm, C = class
#CHECKING LOADED DATA
irisData
ggplot(irisData, aes(SL, SW, color = C)) + geom_point() + labs(color='Class',x ='Sepal length in cm', y ='Sepal width in cm')
ggplot(irisData, aes(PL, PW, color = C)) + geom_point() + labs(color='Class',x ='Petal length in cm', y ='Petal width in cm')
#KMEAN sepal
set.seed(as.numeric(Sys.time()))
irisClusterSepal <- kmeans(irisData[,1:2], length(unique(irisData[,5])))
irisClusterSepal
table(irisClusterSepal$cluster, irisData$C)
irisClusterSepal$cluster <- as.factor(irisClusterSepal$cluster)
ggplot(irisData, aes(SL, SW, color = irisClusterSepal$cluster,shape=C)) + geom_point() +labs(color='Cluster',shape='Class',x ='Sepal length in cm', y ='Sepal width in cm')
#KMEAN petal
set.seed(as.numeric(Sys.time()))
irisClusterPetal <- kmeans(irisData[,3:4], length(unique(irisData[,5])))
irisClusterPetal
table(irisClusterPetal$cluster, irisData$C)
irisClusterPetal$cluster <- as.factor(irisClusterPetal$cluster)
ggplot(irisData, aes(PL, PW, color = irisClusterPetal$cluster,shape=C)) + geom_point() +labs(color='Cluster',shape='Class',x ='Petal length in cm', y ='Petal width in cm')
#KMEAN petal and sepal
set.seed(as.numeric(Sys.time()))
irisCluster <- kmeans(irisData[,1:4], length(unique(irisData[,5])))
irisCluster
table(irisCluster$cluster, irisData$C)
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(irisData, aes(SL, SW, color = irisCluster$cluster,shape=C)) + geom_point() +labs(color='Cluster', shape='Class',x ='Sepal length in cm', y ='Sepal width in cm')
ggplot(irisData, aes(PL, PW, color = irisCluster$cluster,shape=C)) + geom_point() +labs(color='Cluster', shape='Class',x ='Petal length in cm', y ='Petal width in cm')
# kNN
normalize <- function(x) { return ((x - min(x)) / (max(x) - min(x))) }
iris_normalized <- as.data.frame(lapply(irisData[1:4], normalize))
summary(iris_normalized$sepal_length) # check if normalized
# creating training data from normalized dataframe
iris_train <- rbind(iris_normalized[1:25,], iris_normalized[51:75,],  iris_normalized[101:125,])
iris_train_labels <- c(irisData[1:25,5], irisData[51:75,5],  irisData[101:125,5]) # create a vector of training labels
iris_test <- rbind(iris_normalized[26:50,], iris_normalized[76:100,],  iris_normalized[126:150,])
iris_test_labels <- c(irisData[26:50,5], irisData[76:100,5],  irisData[126:150,5]) # create a vector of test labels
# creating predictions with k=sqrt(total values)
iris_test_pred <- knn(train = iris_train, test = iris_test,cl = iris_train_labels, k=sqrt(nrow(irisData)))
CrossTable(x = iris_test_labels, y = iris_test_pred, prop.chisq = FALSE) # print cross table to evaluate predictions
plot.df = data.frame(iris_test, iris_test_pred)
plot.df1 = data.frame(x = plot.df$Sepal.Length,
y = plot.df$Sepal.Width,
predicted = plot.df$predicted)
find_hull = function(df) df[chull(df$x, df$y), ]
boundary = ddply(plot.df1, .variables = "predicted", .fun = find_hull)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 1) +
geom_polygon(data = boundary, aes(x,y), alpha = 0.5)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 1) +
geom_polygon(data = boundary, aes(SL,SW), alpha = 0.5)
ggplot(plot.df, aes(SL, SW, color = iris_test_pred, fill = iris_test_pred)) +
geom_point(size = 1) +
geom_polygon(data = boundary, aes(x=SL,y=SW), alpha = 0.5)
library(ggplot2) #Import ggplot2 library
library(class)
library(gmodels)
rm(list = ls()) #Clean all the old variables
set.seed(as.numeric(Sys.time())) #Initialization of seed with timestamp to get real random every execution
#LOADING DATASET
irisData = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data",header=FALSE,sep = ",") #Loading the dataset in the variable irisData
names(irisData) <- c("SL", "SW" , "PL", "PW", "C" ) #Renaming the colums: SL = sepal length in cm, SW = sepal width in cm, PL = petal length in cm, PW = petal width in cm, C = class
#CHECKING LOADED DATA
irisData
ggplot(irisData, aes(SL, SW, color = C)) + geom_point() + labs(color='Class',x ='Sepal length in cm', y ='Sepal width in cm')
ggplot(irisData, aes(PL, PW, color = C)) + geom_point() + labs(color='Class',x ='Petal length in cm', y ='Petal width in cm')
#KMEAN sepal
set.seed(as.numeric(Sys.time()))
irisClusterSepal <- kmeans(irisData[,1:2], length(unique(irisData[,5])))
irisClusterSepal
table(irisClusterSepal$cluster, irisData$C)
irisClusterSepal$cluster <- as.factor(irisClusterSepal$cluster)
ggplot(irisData, aes(SL, SW, color = irisClusterSepal$cluster,shape=C)) + geom_point() +labs(color='Cluster',shape='Class',x ='Sepal length in cm', y ='Sepal width in cm')
#KMEAN petal
set.seed(as.numeric(Sys.time()))
irisClusterPetal <- kmeans(irisData[,3:4], length(unique(irisData[,5])))
irisClusterPetal
table(irisClusterPetal$cluster, irisData$C)
irisClusterPetal$cluster <- as.factor(irisClusterPetal$cluster)
ggplot(irisData, aes(PL, PW, color = irisClusterPetal$cluster,shape=C)) + geom_point() +labs(color='Cluster',shape='Class',x ='Petal length in cm', y ='Petal width in cm')
#KMEAN petal and sepal
set.seed(as.numeric(Sys.time()))
irisCluster <- kmeans(irisData[,1:4], length(unique(irisData[,5])))
irisCluster
table(irisCluster$cluster, irisData$C)
irisCluster$cluster <- as.factor(irisCluster$cluster)
ggplot(irisData, aes(SL, SW, color = irisCluster$cluster,shape=C)) + geom_point() +labs(color='Cluster', shape='Class',x ='Sepal length in cm', y ='Sepal width in cm')
ggplot(irisData, aes(PL, PW, color = irisCluster$cluster,shape=C)) + geom_point() +labs(color='Cluster', shape='Class',x ='Petal length in cm', y ='Petal width in cm')
# kNN
normalize <- function(x) { return ((x - min(x)) / (max(x) - min(x))) }
decisionplot <- function(model, data, class = NULL, predict_type = "class",
resolution = 100, showgrid = TRUE, ...) {
if(!is.null(class)) cl <- data[,class] else cl <- 1
data <- data[,1:2]
k <- length(unique(cl))
plot(data, col = as.integer(cl)+1L, pch = as.integer(cl)+1L, ...)
# make grid
r <- sapply(data, range, na.rm = TRUE)
xs <- seq(r[1,1], r[2,1], length.out = resolution)
ys <- seq(r[1,2], r[2,2], length.out = resolution)
g <- cbind(rep(xs, each=resolution), rep(ys, time = resolution))
colnames(g) <- colnames(r)
g <- as.data.frame(g)
### guess how to get class labels from predict
### (unfortunately not very consistent between models)
p <- predict(model, g, type = predict_type)
if(is.list(p)) p <- p$class
p <- as.factor(p)
if(showgrid) points(g, col = as.integer(p)+1L, pch = ".")
z <- matrix(as.integer(p), nrow = resolution, byrow = TRUE)
contour(xs, ys, z, add = TRUE, drawlabels = FALSE,
lwd = 2, levels = (1:(k-1))+.5)
invisible(z)
}
iris_normalized <- as.data.frame(lapply(irisData[1:4], normalize))
summary(iris_normalized$sepal_length) # check if normalized
# creating training data from normalized dataframe
iris_train <- rbind(iris_normalized[1:25,], iris_normalized[51:75,],  iris_normalized[101:125,])
iris_train_labels <- c(irisData[1:25,5], irisData[51:75,5],  irisData[101:125,5]) # create a vector of training labels
iris_test <- rbind(iris_normalized[26:50,], iris_normalized[76:100,],  iris_normalized[126:150,])
iris_test_labels <- c(irisData[26:50,5], irisData[76:100,5],  irisData[126:150,5]) # create a vector of test labels
# creating predictions with k=sqrt(total values)
iris_test_pred <- knn(train = iris_train, test = iris_test,cl = iris_train_labels, k=sqrt(nrow(irisData)))
CrossTable(x = iris_test_labels, y = iris_test_pred, prop.chisq = FALSE) # print cross table to evaluate predictions
decisionplot(iris_test_pred, iris_test, class = "Species", main = "kNN (1)")
decisionplot(iris_test_pred, iris_test, class = "Species", main = "kNN (1)")
decisionplot(iris_test_pred, iris_train, class = "Species", main = "kNN (1)")
decisionplot(iris_test_pred, iris_train, class = "1", main = "kNN (1)")
